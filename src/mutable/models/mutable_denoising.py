# Copyright (c) 2025 brineylab @ scripps
# Distributed under the terms of the MIT License.
# SPDX-License-Identifier: MIT

from typing import Optional, Union

import torch
import torch.nn as nn

from ..config import MutableConfig
from ..outputs import DenoisingOutput
from .base import MutablePreTrainedModel, FreezeBaseModelMixin, ParameterCountMixin
from .mutable import MutableModel

__all__ = ["MutableForDenoising"]


class MutableForDenoising(
    MutablePreTrainedModel, FreezeBaseModelMixin, ParameterCountMixin
):
    """
    Phase 1: Mutable model with denoising (BART-style) language modeling head.

    Takes corrupted input, encodes through bottleneck, decodes with teacher forcing,
    and predicts the original (clean) sequence.

    Parameters
    ----------
    config : MutableConfig
        Model configuration.
    """

    config_class = MutableConfig
    base_model_prefix = "mutable"

    def __init__(self, config: MutableConfig):
        super().__init__(config)
        self.config = config

        self.mutable = MutableModel(config)
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)
        self.criterion = nn.CrossEntropyLoss(ignore_index=-100)

        self.init_weights()

    def _shift_right(self, input_ids: torch.LongTensor) -> torch.LongTensor:
        """
        Shift input IDs right for teacher forcing.
        Prepend BOS token and drop the last token.
        """
        shifted = input_ids.new_zeros(input_ids.shape)
        shifted[:, 1:] = input_ids[:, :-1].clone()
        shifted[:, 0] = self.config.bos_token_id
        # replace potential -100 values in labels for decoder input
        shifted.masked_fill_(shifted == -100, self.config.pad_token_id)
        return shifted

    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.LongTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        decoder_input_ids: Optional[torch.LongTensor] = None,
        decoder_attention_mask: Optional[torch.LongTensor] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
    ) -> Union[DenoisingOutput, tuple]:
        """
        Parameters
        ----------
        input_ids : torch.LongTensor
            Corrupted input token IDs (encoder input).
        attention_mask : torch.LongTensor
            Attention mask for encoder input.
        labels : torch.LongTensor
            Clean target token IDs for loss computation.
        decoder_input_ids : torch.LongTensor, optional
            If not provided, auto-generated by shifting labels right.
        """
        return_dict = (
            return_dict if return_dict is not None else self.config.return_dict
        )

        # auto-generate decoder input from labels if not provided
        if decoder_input_ids is None and labels is not None:
            decoder_input_ids = self._shift_right(labels)

        # encode + bottleneck + decode
        outputs = self.mutable(
            input_ids=input_ids,
            attention_mask=attention_mask,
            decoder_input_ids=decoder_input_ids,
            decoder_attention_mask=decoder_attention_mask,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=True,
        )

        # LM head
        lm_logits = self.lm_head(outputs.last_hidden_state)

        # loss
        loss = None
        if labels is not None:
            labels = labels.to(lm_logits.device)
            loss = self.criterion(
                lm_logits.view(-1, self.config.vocab_size),
                labels.view(-1),
            )

        if not return_dict:
            return tuple(
                v
                for v in [loss, lm_logits, outputs.latent_states]
                if v is not None
            )
        return DenoisingOutput(
            loss=loss,
            logits=lm_logits,
            latent_states=outputs.latent_states,
            encoder_last_hidden_state=outputs.encoder_last_hidden_state,
            encoder_attentions=outputs.encoder_attentions,
        )
