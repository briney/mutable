vocab_size: 32
hidden_size: 192
num_encoder_layers: 4
num_decoder_layers: 4
num_attention_heads: 12
intermediate_size: null
activation: swiglu
position_embedding_type: rotary
num_latents: 16
latent_dim: null
dropout: 0.1
attention_dropout: null
hidden_dropout: null
ffn_bias: true
max_position_embeddings: 512
initializer_range: 0.02
layer_norm_eps: 1.0e-5
pad_token_id: 1
bos_token_id: 0
eos_token_id: 2
sep_token_id: 29
mask_token_id: 31
